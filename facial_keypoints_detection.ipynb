{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b85fa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# IMPORT LIBRARIES\n",
    "# =========================\n",
    "import zipfile, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# For reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# =========================\n",
    "# SETUP PATHS AND UNZIP DATA\n",
    "# =========================\n",
    "base_path = \"/kaggle/input/facial-keypoints-detection\"\n",
    "work_path = \"/kaggle/working\"\n",
    "\n",
    "# Unzip training.zip\n",
    "with zipfile.ZipFile(f\"{base_path}/training.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(work_path)\n",
    "\n",
    "# Unzip test.zip\n",
    "with zipfile.ZipFile(f\"{base_path}/test.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(work_path)\n",
    "\n",
    "print(\"Extracted files:\", os.listdir(work_path))\n",
    "\n",
    "# =========================\n",
    "# LOAD CSV FILES\n",
    "# =========================\n",
    "TRAIN_CSV = f\"{work_path}/training.csv\"\n",
    "TEST_CSV  = f\"{work_path}/test.csv\"\n",
    "LOOKUP_CSV = f\"{base_path}/IdLookupTable.csv\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "test  = pd.read_csv(TEST_CSV)\n",
    "lookup = pd.read_csv(LOOKUP_CSV)\n",
    "\n",
    "print(\"TRAIN shape:\", train.shape)\n",
    "print(\"TEST shape:\", test.shape)\n",
    "print(\"LOOKUP shape:\", lookup.shape)\n",
    "\n",
    "# =========================\n",
    "# HELPER FUNCTIONS\n",
    "# =========================\n",
    "def image_from_str(img_str):\n",
    "    arr = np.fromstring(img_str, sep=' ')\n",
    "    return arr.reshape(96, 96)\n",
    "\n",
    "def prepare_images(df):\n",
    "    imgs = np.stack(df['Image'].apply(lambda s: image_from_str(s)).values)\n",
    "    imgs = imgs.astype('float32') / 255.0\n",
    "    imgs = imgs.reshape(-1, 96, 96, 1)\n",
    "    return imgs\n",
    "\n",
    "# =========================\n",
    "# VISUALIZE SAMPLE IMAGE\n",
    "# =========================\n",
    "sample = train.dropna().iloc[0]\n",
    "img = image_from_str(sample['Image'])\n",
    "plt.imshow(img, cmap='gray')\n",
    "kp = sample.drop('Image').values.astype(float)\n",
    "plt.scatter(kp[0::2], kp[1::2], c='r', s=10)\n",
    "plt.title(\"Sample image with keypoints\")\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# CLEAN AND PREPARE DATA\n",
    "# =========================\n",
    "train_clean = train.dropna().reset_index(drop=True)\n",
    "print(\"After dropna:\", train_clean.shape)\n",
    "\n",
    "X = prepare_images(train_clean)\n",
    "y = train_clean.drop(columns=['Image']).values.astype('float32')\n",
    "kp_columns = train_clean.drop(columns=['Image']).columns.tolist()\n",
    "print(\"Keypoint columns:\", kp_columns)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=SEED\n",
    ")\n",
    "print(\"X_train shape:\", X_train.shape, \"X_val shape:\", X_val.shape)\n",
    "\n",
    "# =========================\n",
    "# BUILD CNN MODEL\n",
    "# =========================\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(96, 96, 1)),\n",
    "        Conv2D(32, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(64, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Conv2D(128, (3,3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2,2),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(len(kp_columns))  # 30 keypoints\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# =========================\n",
    "# TRAIN MODEL\n",
    "# =========================\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=80,\n",
    "    batch_size=64,\n",
    "    callbacks=[es, mc],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Plot training / validation loss\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training / Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# =========================\n",
    "# PREPARE TEST DATA AND PREDICT\n",
    "# =========================\n",
    "test_images = prepare_images(test)\n",
    "model.load_weights('best_model.h5')\n",
    "preds = model.predict(test_images)\n",
    "print(\"Predictions shape:\", preds.shape)  # should be (n_test, 30)\n",
    "\n",
    "# =========================\n",
    "# CREATE CORRECT SUBMISSION (CLIPPED TO [0,96])\n",
    "# =========================\n",
    "submission = lookup.copy()\n",
    "\n",
    "submission['Location'] = submission.apply(\n",
    "    lambda r: np.clip(\n",
    "        preds[int(r['ImageId']) - 1, kp_columns.index(r['FeatureName'])],\n",
    "        0, 96  # clip to valid image coordinates\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Keep only RowId and Location columns\n",
    "submission = submission[['RowId', 'Location']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"âœ… submission.csv ready for Kaggle (coordinates clipped to [0,96])!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
